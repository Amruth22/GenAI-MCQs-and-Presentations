# MCQ Questions - Frameworks & Evaluation Methods

## Instructions
Choose the best answer for each question. Each question has only one correct answer.

---

### Question 1
What is Hugging Face primarily known for?

A) Hardware acceleration  
B) Open-source machine learning platform with pre-trained models and datasets  
C) Database management  
D) Web development frameworks  

**Answer: B**

---

### Question 2
What is the main purpose of LangChain?

A) Language translation only  
B) Framework for developing applications with large language models  
C) Data visualization  
D) Model training acceleration  

**Answer: B**

---

### Question 3
What does LlamaIndex specialize in?

A) Model training  
B) Data indexing and retrieval for LLM applications  
C) Image processing  
D) Audio analysis  

**Answer: B**

---

### Question 4
What does BLEU score measure?

A) Model training speed  
B) Quality of machine translation by comparing with reference translations  
C) Memory usage  
D) Model size  

**Answer: B**

---

### Question 5
What does ROUGE score evaluate?

A) Translation quality  
B) Quality of text summarization by measuring overlap with reference summaries  
C) Model accuracy  
D) Training time  

**Answer: B**

---

### Question 6
What is the Hugging Face Transformers library used for?

A) Data storage  
B) Easy access to pre-trained transformer models for various NLP tasks  
C) Web scraping  
D) Database queries  

**Answer: B**

---

### Question 7
What is a key advantage of using LangChain for LLM applications?

A) Faster training  
B) Modular components for chaining LLM operations and integrating external tools  
C) Smaller model sizes  
D) Better graphics  

**Answer: B**

---

### Question 8
What is the primary function of LlamaIndex's vector stores?

A) Storing images  
B) Efficient storage and retrieval of document embeddings for semantic search  
C) Caching web pages  
D) Storing user preferences  

**Answer: B**

---

### Question 9
What is a limitation of BLEU scores?

A) Too fast to compute  
B) Focuses on n-gram overlap and may miss semantic meaning  
C) Uses too much memory  
D) Only works with English  

**Answer: B**

---

### Question 10
What does human evaluation provide that automated metrics cannot?

A) Faster results  
B) Subjective assessment of quality, coherence, and appropriateness  
C) Lower costs  
D) Consistent scoring  

**Answer: B**

---

### Question 11
What is the Hugging Face Model Hub?

A) A hardware component  
B) A repository for sharing and discovering pre-trained models  
C) A training algorithm  
D) A data preprocessing tool  

**Answer: B**

---

### Question 12
What does LangChain's memory component enable?

A) Faster processing  
B) Maintaining context and conversation history across interactions  
C) Data compression  
D) Model optimization  

**Answer: B**

---

### Question 13
What is the purpose of LlamaIndex's query engines?

A) Database management  
B) Processing natural language queries over indexed data  
C) Image recognition  
D) Audio processing  

**Answer: B**

---

### Question 14
What does ROUGE-L measure specifically?

A) Word frequency  
B) Longest common subsequence between generated and reference text  
C) Model parameters  
D) Training epochs  

**Answer: B**

---

### Question 15
What is a key challenge in human evaluation of LLM outputs?

A) Too fast to complete  
B) Subjectivity, cost, and scalability issues  
C) Too accurate  
D) Automated process  

**Answer: B**

---

### Question 16
What does the Hugging Face Datasets library provide?

A) Model architectures  
B) Easy access to thousands of datasets for machine learning  
C) Hardware optimization  
D) Web interfaces  

**Answer: B**

---

### Question 17
What is LangChain's agent framework designed for?

A) Data storage  
B) Creating autonomous systems that can use tools and make decisions  
C) Image processing  
D) Database queries  

**Answer: B**

---

### Question 18
What does LlamaIndex's document loading capability handle?

A) Only text files  
B) Various document formats including PDFs, web pages, and databases  
C) Only images  
D) Only audio files  

**Answer: B**

---

### Question 19
What is the range of BLEU scores?

A) -1 to 1  
B) 0 to 1 (or 0 to 100 when expressed as percentage)  
C) 0 to infinity  
D) -100 to 100  

**Answer: B**

---

### Question 20
What does inter-annotator agreement measure in human evaluation?

A) Speed of annotation  
B) Consistency between different human evaluators  
C) Cost of evaluation  
D) Automated accuracy  

**Answer: B**

---

### Question 21
What is Hugging Face Spaces used for?

A) Data storage  
B) Hosting and sharing machine learning demos and applications  
C) Model training  
D) Database management  

**Answer: B**

---

### Question 22
What does LangChain's document loader interface standardize?

A) Model outputs  
B) Loading documents from various sources into a common format  
C) Training procedures  
D) Evaluation metrics  

**Answer: B**

---

### Question 23
What is LlamaIndex's chat engine designed for?

A) Social media  
B) Conversational interfaces over indexed data  
C) Gaming  
D) Video streaming  

**Answer: B**

---

### Question 24
What does ROUGE-N measure?

A) Model size  
B) N-gram overlap between generated and reference summaries  
C) Training time  
D) Memory usage  

**Answer: B**

---

### Question 25
What is a key benefit of using pre-trained models from Hugging Face?

A) Slower inference  
B) Reduced training time and computational resources for downstream tasks  
C) Larger model sizes  
D) More complex setup  

**Answer: B**

---

### Question 26
What does LangChain's output parser help with?

A) Input preprocessing  
B) Structuring and formatting LLM responses into desired formats  
C) Model training  
D) Data storage  

**Answer: B**

---

### Question 27
What is the purpose of LlamaIndex's response synthesizers?

A) Creating training data  
B) Combining retrieved information to generate comprehensive answers  
C) Model compression  
D) Data validation  

**Answer: B**

---

### Question 28
What does semantic similarity evaluation measure that BLEU/ROUGE might miss?

A) Processing speed  
B) Meaning preservation even with different word choices  
C) Model size  
D) Training cost  

**Answer: B**

---

### Question 29
What is the Hugging Face Tokenizers library optimized for?

A) Model training  
B) Fast and efficient text tokenization for transformer models  
C) Data visualization  
D) Web scraping  

**Answer: B**

---

### Question 30
What does end-to-end evaluation assess in LLM applications?

A) Individual components only  
B) Overall system performance including all integrated components  
C) Training data quality  
D) Hardware requirements  

**Answer: B**

---

## Answer Key Summary
1. B  2. B  3. B  4. B  5. B  
6. B  7. B  8. B  9. B  10. B  
11. B  12. B  13. B  14. B  15. B  
16. B  17. B  18. B  19. B  20. B  
21. B  22. B  23. B  24. B  25. B  
26. B  27. B  28. B  29. B  30. B  

---

**Total Questions: 30**  
**Topics Covered:** Hugging Face, LangChain, LlamaIndex, BLEU/ROUGE Metrics, Human Evaluation, and Framework Integration