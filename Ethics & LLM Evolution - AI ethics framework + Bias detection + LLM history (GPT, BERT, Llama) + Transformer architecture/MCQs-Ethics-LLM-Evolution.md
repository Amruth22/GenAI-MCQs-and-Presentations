# MCQ Questions - Ethics & LLM Evolution

## Instructions
Choose the best answer for each question. Each question has only one correct answer.

---

### Question 1
What is the primary goal of AI ethics frameworks?

A) To increase model performance  
B) To ensure responsible and fair development and deployment of AI systems  
C) To reduce computational costs  
D) To speed up training processes  

**Answer: B**

---

### Question 2
What does algorithmic bias refer to?

A) Preference for certain algorithms  
B) Systematic and unfair discrimination in AI system outputs  
C) Computational efficiency differences  
D) Model accuracy variations  

**Answer: B**

---

### Question 3
What was the key innovation introduced by the Transformer architecture?

A) Convolutional layers  
B) Self-attention mechanism for parallel processing  
C) Recurrent connections  
D) Pooling operations  

**Answer: B**

---

### Question 4
Which model introduced the concept of bidirectional training for language understanding?

A) GPT  
B) BERT (Bidirectional Encoder Representations from Transformers)  
C) T5  
D) LLaMA  

**Answer: B**

---

### Question 5
What does the attention mechanism in Transformers allow the model to do?

A) Reduce memory usage  
B) Focus on relevant parts of the input sequence when processing each token  
C) Speed up training  
D) Compress data  

**Answer: B**

---

### Question 6
What is a key principle of responsible AI development?

A) Maximizing profit  
B) Transparency, accountability, and fairness  
C) Fastest deployment  
D) Minimal testing  

**Answer: B**

---

### Question 7
What type of bias occurs when training data is not representative of the target population?

A) Confirmation bias  
B) Selection bias or sampling bias  
C) Cognitive bias  
D) Measurement bias  

**Answer: B**

---

### Question 8
What distinguishes GPT models from BERT models?

A) GPT uses attention, BERT doesn't  
B) GPT is autoregressive (predicts next token), BERT is bidirectional (masked language modeling)  
C) GPT is smaller than BERT  
D) BERT came before GPT  

**Answer: B**

---

### Question 9
What does "fairness" mean in the context of AI ethics?

A) Equal computational resources for all users  
B) Equitable treatment and outcomes across different groups and individuals  
C) Same model architecture for all applications  
D) Identical training data for all models  

**Answer: B**

---

### Question 10
What is positional encoding used for in Transformers?

A) To reduce model size  
B) To provide information about token positions since Transformers lack inherent sequence order  
C) To speed up computation  
D) To handle different languages  

**Answer: B**

---

### Question 11
What was a major limitation of pre-Transformer models like RNNs for language processing?

A) They were too accurate  
B) Sequential processing prevented parallelization and caused vanishing gradients  
C) They used too little memory  
D) They were too fast  

**Answer: B**

---

### Question 12
What does LLaMA stand for?

A) Large Language Model Architecture  
B) Large Language Model Meta AI  
C) Linear Language Modeling Approach  
D) Learned Language Memory Algorithm  

**Answer: B**

---

### Question 13
What is the purpose of multi-head attention in Transformers?

A) To reduce computational cost  
B) To allow the model to attend to different types of relationships simultaneously  
C) To make models smaller  
D) To eliminate the need for training data  

**Answer: B**

---

### Question 14
What ethical concern is raised by the environmental impact of large language models?

A) Data privacy only  
B) High energy consumption and carbon footprint from training and inference  
C) Model accuracy  
D) Training speed  

**Answer: B**

---

### Question 15
What is the main difference between encoder-only and decoder-only Transformer architectures?

A) Encoder-only models are smaller  
B) Encoder-only models are for understanding tasks, decoder-only for generation tasks  
C) Decoder-only models are older  
D) There is no difference  

**Answer: B**

---

### Question 16
What does "AI alignment" refer to in ethics discussions?

A) Organizing AI models in databases  
B) Ensuring AI systems behave according to human values and intentions  
C) Aligning computational resources  
D) Synchronizing multiple models  

**Answer: B**

---

### Question 17
What technique does BERT use during pre-training?

A) Next sentence prediction only  
B) Masked Language Modeling and Next Sentence Prediction  
C) Image classification  
D) Reinforcement learning  

**Answer: B**

---

### Question 18
What is a key method for detecting bias in AI systems?

A) Increasing model size  
B) Analyzing performance disparities across different demographic groups  
C) Using more training data  
D) Changing the architecture  

**Answer: B**

---

### Question 19
What was the original Transformer model primarily designed for?

A) Image recognition  
B) Machine translation  
C) Speech recognition  
D) Game playing  

**Answer: B**

---

### Question 20
What does "explainable AI" aim to achieve?

A) Faster model training  
B) Making AI decision-making processes interpretable and understandable  
C) Reducing model size  
D) Increasing accuracy  

**Answer: B**

---

### Question 21
What is the key innovation of GPT-3 compared to earlier GPT models?

A) Different architecture  
B) Massive scale (175 billion parameters) enabling few-shot learning  
C) Faster training  
D) Lower computational requirements  

**Answer: B**

---

### Question 22
What does "data poisoning" refer to in AI security?

A) Corrupted storage devices  
B) Maliciously manipulating training data to compromise model behavior  
C) Using too much data  
D) Slow data loading  

**Answer: B**

---

### Question 23
What is the purpose of layer normalization in Transformers?

A) To reduce model size  
B) To stabilize training and improve convergence  
C) To add more parameters  
D) To slow down training  

**Answer: B**

---

### Question 24
What ethical principle emphasizes the need for AI systems to be understandable?

A) Efficiency  
B) Transparency and explainability  
C) Speed  
D) Complexity  

**Answer: B**

---

### Question 25
What is "prompt engineering" in the context of large language models?

A) Building hardware for AI  
B) Crafting input prompts to elicit desired responses from language models  
C) Engineering training datasets  
D) Designing model architectures  

**Answer: B**

---

### Question 26
What does "model interpretability" help address in AI ethics?

A) Training speed  
B) Understanding how models make decisions to ensure fairness and accountability  
C) Model size  
D) Data storage  

**Answer: B**

---

### Question 27
What is a key characteristic of autoregressive language models like GPT?

A) They process all tokens simultaneously  
B) They generate text by predicting one token at a time based on previous tokens  
C) They only work with images  
D) They require labeled data  

**Answer: B**

---

### Question 28
What does "AI governance" encompass?

A) Only technical aspects  
B) Policies, regulations, and frameworks for responsible AI development and deployment  
C) Only legal aspects  
D) Only business aspects  

**Answer: B**

---

### Question 29
What is the significance of the "attention is all you need" paper?

A) It introduced RNNs  
B) It demonstrated that attention mechanisms alone could achieve state-of-the-art results  
C) It proved CNNs were superior  
D) It introduced backpropagation  

**Answer: B**

---

### Question 30
What is a key consideration for bias mitigation in AI systems?

A) Using more computational power  
B) Diverse and representative training data, regular bias audits, and inclusive development teams  
C) Faster training algorithms  
D) Larger model sizes  

**Answer: B**

---

## Answer Key Summary
1. B  2. B  3. B  4. B  5. B  
6. B  7. B  8. B  9. B  10. B  
11. B  12. B  13. B  14. B  15. B  
16. B  17. B  18. B  19. B  20. B  
21. B  22. B  23. B  24. B  25. B  
26. B  27. B  28. B  29. B  30. B  

---

**Total Questions: 30**  
**Topics Covered:** AI Ethics Framework, Bias Detection, LLM History (GPT, BERT, LLaMA), Transformer Architecture, Responsible AI, and Model Interpretability