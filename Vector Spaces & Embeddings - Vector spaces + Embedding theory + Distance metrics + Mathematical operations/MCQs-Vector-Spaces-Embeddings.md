# MCQ Questions - Vector Spaces & Embeddings

## Instructions
Choose the best answer for each question. Each question has only one correct answer.

---

### Question 1
What is a vector space?

A) A physical space containing vectors  
B) A mathematical structure with vectors that can be added and scaled  
C) A computer memory allocation for storing vectors  
D) A graphical representation of data points  

**Answer: B**

---

### Question 2
What does linear independence mean in vector spaces?

A) Vectors that are parallel to each other  
B) No vector in the set can be expressed as a linear combination of others  
C) Vectors that have the same magnitude  
D) Vectors that point in opposite directions  

**Answer: B**

---

### Question 3
What is the purpose of word embeddings in NLP?

A) To compress text files  
B) To represent words as dense numerical vectors capturing semantic meaning  
C) To count word frequencies  
D) To translate between languages  

**Answer: B**

---

### Question 4
Which distance metric measures the angle between two vectors?

A) Euclidean distance  
B) Cosine similarity  
C) Manhattan distance  
D) Hamming distance  

**Answer: B**

---

### Question 5
What is the dimensionality of a vector space?

A) The physical size of vectors  
B) The number of linearly independent vectors in a basis  
C) The total number of vectors in the space  
D) The computational complexity  

**Answer: B**

---

### Question 6
What does the dot product of two vectors represent?

A) The sum of their components  
B) A scalar value indicating their similarity and alignment  
C) A new vector perpendicular to both  
D) The distance between vector endpoints  

**Answer: B**

---

### Question 7
Which embedding technique uses neural networks to learn word representations?

A) Bag of Words  
B) Word2Vec  
C) TF-IDF  
D) One-hot encoding  

**Answer: B**

---

### Question 8
What is the range of cosine similarity values?

A) 0 to 1  
B) -1 to 1  
C) 0 to infinity  
D) -infinity to infinity  

**Answer: B**

---

### Question 9
What does Euclidean distance measure?

A) The angle between vectors  
B) The straight-line distance between two points in space  
C) The correlation between vectors  
D) The number of different components  

**Answer: B**

---

### Question 10
What is a basis in a vector space?

A) The origin point of the space  
B) A set of linearly independent vectors that span the entire space  
C) The largest vector in the space  
D) The average of all vectors  

**Answer: B**

---

### Question 11
What is the main advantage of dense embeddings over sparse representations?

A) They use more memory  
B) They capture semantic relationships and require less storage  
C) They are easier to interpret  
D) They work only with small datasets  

**Answer: B**

---

### Question 12
What does Manhattan distance calculate?

A) The shortest path between two points  
B) The sum of absolute differences between corresponding coordinates  
C) The angle between two vectors  
D) The correlation coefficient  

**Answer: B**

---

### Question 13
What is the curse of dimensionality?

A) Vectors becoming too large  
B) Performance degradation and sparsity issues in high-dimensional spaces  
C) Inability to visualize data  
D) Computational speed improvements  

**Answer: B**

---

### Question 14
What does vector normalization achieve?

A) Removes negative values  
B) Scales vectors to unit length while preserving direction  
C) Sorts vector components  
D) Converts vectors to integers  

**Answer: B**

---

### Question 15
What is the purpose of Principal Component Analysis (PCA)?

A) To increase dimensionality  
B) To reduce dimensionality while preserving maximum variance  
C) To normalize vectors  
D) To calculate distances  

**Answer: B**

---

### Question 16
What does the term "embedding space" refer to?

A) Physical storage location  
B) The vector space where embedded representations exist  
C) The original data format  
D) The computational time required  

**Answer: B**

---

### Question 17
What is a key property of orthogonal vectors?

A) They have the same direction  
B) Their dot product is zero  
C) They have equal magnitude  
D) They are parallel  

**Answer: B**

---

### Question 18
What does semantic similarity in embeddings measure?

A) Syntactic structure  
B) How close words are in meaning based on their vector representations  
C) Alphabetical order  
D) Word frequency  

**Answer: B**

---

### Question 19
What is the Skip-gram model used for?

A) Text classification  
B) Learning word embeddings by predicting context words  
C) Image processing  
D) Data compression  

**Answer: B**

---

### Question 20
What does vector addition represent geometrically?

A) Multiplication of magnitudes  
B) Placing vectors head-to-tail to find the resultant vector  
C) Finding the angle between vectors  
D) Calculating the distance  

**Answer: B**

---

### Question 21
What is the difference between L1 and L2 norms?

A) L1 uses squares, L2 uses absolute values  
B) L1 uses absolute values, L2 uses squares (then square root)  
C) They are identical  
D) L1 is for 2D, L2 is for 3D  

**Answer: B**

---

### Question 22
What does contextual embedding capture that static embedding cannot?

A) Word frequency  
B) Different meanings of words based on surrounding context  
C) Alphabetical information  
D) Word length  

**Answer: B**

---

### Question 23
What is the purpose of negative sampling in Word2Vec?

A) To remove bad data  
B) To efficiently train the model by sampling negative examples  
C) To create opposite meanings  
D) To reduce vocabulary size  

**Answer: B**

---

### Question 24
What does linear transformation of vectors preserve?

A) Only magnitude  
B) Linear relationships and vector space structure  
C) Only direction  
D) Only the origin  

**Answer: B**

---

### Question 25
What is the key insight behind distributional semantics?

A) Words are defined by their spelling  
B) Words with similar meanings appear in similar contexts  
C) Longer words are more important  
D) Word order doesn't matter  

**Answer: B**

---

### Question 26
What does the magnitude (norm) of a vector represent?

A) Its direction  
B) Its length or size in the vector space  
C) Its position  
D) Its angle  

**Answer: B**

---

### Question 27
What is t-SNE primarily used for?

A) Training embeddings  
B) Visualizing high-dimensional data in 2D or 3D space  
C) Calculating distances  
D) Normalizing vectors  

**Answer: B**

---

### Question 28
What does subspace mean in vector spaces?

A) A smaller physical area  
B) A subset of vectors that forms a vector space under the same operations  
C) Negative vector values  
D) Vectors below average magnitude  

**Answer: B**

---

### Question 29
What is the relationship between cosine similarity and Euclidean distance for normalized vectors?

A) They are unrelated  
B) They are inversely related and can be converted between each other  
C) They are identical  
D) Cosine similarity is always larger  

**Answer: B**

---

### Question 30
What does the term "vector quantization" refer to?

A) Counting vectors  
B) Mapping continuous vectors to discrete representative vectors  
C) Measuring vector quality  
D) Sorting vectors by magnitude  

**Answer: B**

---

## Answer Key Summary
1. B  2. B  3. B  4. B  5. B  
6. B  7. B  8. B  9. B  10. B  
11. B  12. B  13. B  14. B  15. B  
16. B  17. B  18. B  19. B  20. B  
21. B  22. B  23. B  24. B  25. B  
26. B  27. B  28. B  29. B  30. B  

---

**Total Questions: 30**  
**Topics Covered:** Vector Spaces, Word Embeddings, Distance Metrics, Mathematical Operations, Dimensionality Reduction, and Semantic Similarity